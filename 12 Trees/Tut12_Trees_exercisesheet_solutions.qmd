---
title: "Tutorial 12 - Tree-based Models"
author: "Victoria HÃ¼newaldt"
format: html
editor: visual
---

# Tutorial 12

This tutorial will cover decision trees.

You will learn:

-   how to run a regression decision tree

-   how to run a classification decision tree

-   how to visualise decision trees

-   how to evaluate its performance on test & training data

# Exercises

Use the airbnbsmall data set. You will need to use the "rpart" and "rpart.plot" library. Create a .qmd-file and solve the tasks there. Store it in the JupyterHub folder "Session 12".

## Regression Decision Tree

Run a regression decision tree explaining the variable "price" (= endogenous variable). Use all other variables as potential predictor variables (i.e. specify a full model).

Print and plot the tree.

```{r}
#| warning: false
#| output: false
#| 
# clean environment 
rm(list=ls())

# load packages/libraries

#install.packages("rpart") 
library(rpart) # for creating trees  
#install.packages("rpart.plot") 
library(rpart.plot) # for plotting trees
#remotes::install_gitlab("BAQ6370/sozoekds", host="gitlab.rrz.uni-hamburg.de")
library(sozoekds) 
library(dplyr)

#load data
airbnb_data <- airbnbsmall # store data as "airbnb_data"

##################

# 1. Regression tree 

regtree <- rpart( # use rpart command to fit a tree
  formula = price~., # full model explaining price
  data=airbnb_data, # using airbnb data
  method = "anova", # method used for conitnuous variables
  #control = list(cp = 0, xval = 10) # would fit uncut tree 
)
# print
regtree
# plot
rpart.plot(regtree)
```

## Classification Decision Tree

Split your data into test and training data. Run a classification decision tree explaining the variable "high_rating" (= endogenous variable) on the training data. Use all other variables as potential predictor variables (i.e. specify a full model).

Create a confusion matrix for the training data and one for the test data.

Compare your results to the results obtained by logistic regression in tutorial 10.

```{r}
#| warning: false
#| output: false

rm(list=ls())

# load packages/libraries

#install.packages("rpart") 
library(rpart) # for creating trees  
#install.packages("rpart.plot") 
library(rpart.plot) # for plotting trees
#remotes::install_gitlab("BAQ6370/sozoekds", host="gitlab.rrz.uni-hamburg.de")
library(sozoekds) 
library(dplyr)
library(caret) # for splitting

# load data
airbnb_data <- airbnbsmall # store data as "airbnb_data"

# binary variable "high rating"
airbnb_data$high_rating = ifelse(airbnb_data$n_review_scores_rating>94, 1, 0) 
airbnb_data_2 <- select(airbnb_data, -n_review_scores_rating) # remove "n_review_scores_rating"

y <- airbnb_data_2$high_rating # defines y as "high_rating" in the airbnb dataset

# split data into test & training
set.seed(123)  # for reproducibility

trainIndex <- createDataPartition(y, p = 0.7, # percentage of data going to training
                                  list = FALSE, 
                                  times = 1) # only 1 split

train <- airbnb_data_2[trainIndex,] # training data
test <- airbnb_data_2[-trainIndex,] # test data

# 2. Classification tree

# train the tree
classtree <- rpart(
  formula = high_rating~.,
  data=train, # this time using training data
  method = "class", # this time using a classification tree as dependent variable is binary
  )

# plot the tree
classtree # prints the tree
rpart.plot(classtree) # plots the tree

# 3. Evaluation

# training confusion matrix
train_predict <- predict(classtree, data=train, type="class")
tab1 <- table(predict = train_predict, actual = train$high_rating)
confusionMatrix(tab1, mode = "prec_recall")


# testing confusion matrix

test_predict <- predict(classtree, newdata=test, type="class")
tab2 <- table(predict = test_predict, actual = test$high_rating)
confusionMatrix(tab2, mode = "prec_recall")
```
