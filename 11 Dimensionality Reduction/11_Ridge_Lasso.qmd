---
title: "Dimensionality reduction - Session 11"
format: 
  html:
    self-contained: true
    theme: minty
    code-block-bg: true
    code-block-border-left: "#31BAE9"
editor: visual
---

# Agenda for today:

1.  Dimensionality problems and ways of solving them
2.  Focus: Shrinkage Methods

# Dimensionality problems and ways of solving them

+-----------------------------------------------------------------------------+--------------------------------------------------------------------+
| Question                                                                    | Solutions                                                          |
|                                                                             |                                                                    |
| *How can we improve the linear model in a world of `many` regressors?*      | -   [Subset selection:]{.underline}                                |
|                                                                             |                                                                    |
| We want to have models with a high accuracy and high model interpretability | -   [Shrinkage methods]{.underline} (also known as regularization) |
|                                                                             |                                                                    |
|                                                                             | -   [Dimensionality reduction]{.underline}                         |
+-----------------------------------------------------------------------------+--------------------------------------------------------------------+

# Shrinkage methods

Shrinkage methods are important tools in machine learning that help create simpler, more generalized models that perform better on new data

-   **Prevent Overfitting**: Shrinkage techniques help models generalize better to unseen data.

-   **Feature Selection**: They can also help in identifying the most important features by reducing the weights of less significant ones.

-   **Trade-off**: There is a balance in choosing the correct penalty term (λ) – too much shrinkage can lead to underfitting (too simple a model), while too little can lead to overfitting. The optimal λ is typically chosen using cross-validation approaches.

# Ridge and Lasso

## Ridge:

$\sum_{i=1}^n (y_i - \hat{\beta_0} - \sum_{j=1}^p \hat{\beta_j} x_{ij})^2 + \lambda \sum_{j=1}^p \hat{\beta_j}^2$

If λ=0 this is OLS regression

If λ→∞ all coefficients go towards zero

## Lasso:

$\sum_{i=1}^n (y_i - \hat{\beta_0} - \sum_{j=1}^p \hat{\beta_j} x_{ij})^2 + \lambda \sum_{j=1}^p |\hat{\beta_j}|$

Certain coefficients are down-weighted to be exactly zero (as in the subset algorithm)

Example in R

## Ridge Step by Step:

1.  Load packages and data

```{r}
#install.packages("glmnet")
library(glmnet)
data(mtcars) #pre-installed dataset
head(mtcars)
```

------------------------------------------------------------------------

2.  Split dataset into target variable (y) and predictors (x1,x2,x3...xn)

    ```{r}
    # Target variable
    y <- mtcars$mpg

    # All other columns as predictors
    x <- as.matrix(mtcars[, -which(names(mtcars) == "mpg")])

    # Check the matrix
    head(x)
    ```

    ------------------------------------------------------------------------

3.  Perform ridge regression

    **Why:** The Ridge Regression technique is used to prevent overfitting, especially when predictor variables are highly correlated (multicollinearity). The \`alpha\` parameter is set to \`0\` for Ridge Regression (1 would indicate Lasso Regression).

    ```{r}
    # Ridge Regression with glmnet
    ridge_model <- glmnet(x, y, alpha = 0)

    # Display summary of the model
    print(ridge_model)
    ```

    **Interpretation**(from `glmnet`-function documentation):

    +-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
    | `lambda`    | The actual sequence of `lambda` values used. When `alpha=0`, the largest lambda reported does not quite give the zero coefficients reported (`lambda=inf` would in principle). Instead, the largest `lambda` for `alpha=0.001` is used, and the sequence of `lambda` values is derived from this.                                                                 |
    +-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
    | `dev.ratio` | The fraction of (null) deviance explained (for `"elnet"`, this is the R-square). The deviance calculations incorporate weights if present in the model. The deviance is defined to be 2\*(loglike_sat - loglike), where loglike_sat is the log-likelihood for the saturated model (a model with a free parameter per observation). Hence dev.ratio=1-dev/nulldev. |
    +-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
    | `df`        | The number of nonzero coefficients for each value of `lambda`. For `"multnet"`, this is the number of variables with a nonzero coefficient for *any* class.                                                                                                                                                                                                       |
    +-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+

    ::: callout-important
    Our table contains the fitted coefficients for different values of the regularization parameter \`lambda\` -\> Still, we need to find the optimal Lambda!
    :::

    ------------------------------------------------------------------------

4.  Find optimal Lambda

    **What we are doing:**

    -   Setting a seed for reproducibility (ensuring consistent results across runs).
    -   Performing cross-validation to find the optimal value of \`lambda\`.
    -   Plotting the cross-validation results.
    -   Extracting and printing the best lambda value.

    **Why:** The choice of \`lambda\` (the regularization strength) is crucial for Ridge Regression. Cross-validation helps determine the best \`lambda\` that minimizes prediction error.

    ```{r}
    set.seed(123) # For reproducibility
    cv_model <- cv.glmnet(x, y, alpha = 0)

    # Plot of cross-validation
    plot(cv_model)

    # Find the optimal lambda value
    best_lambda <- cv_model$lambda.min
    print(best_lambda)
    ```

    **Interpretation:** The plot shows the cross-validated mean square error for different values of lambda. We also have two lines: one for the lambda with the lowest MSE and another one that is the highest lambda within one standard deviation of the lambda with the lowest MSE. We typically choose the second value as the best lambda.

    You would look for the minimum error value and then move vertically up to find the error value plus the one standard error. The corresponding lambda value at that point can be considered as a more conservative alternative to the optimal lambda, which may help avoid overfitting.

    **Visual Guides**: If the plot has vertical markers or lines that indicate the best lambda or one-standard-error lambda, use those as guides for identifying the optimal points.

    ------------------------------------------------------------------------

5.  Retrieve coefficients

    **Why we are doing this:**

    -   In Ridge Regression, the coefficients provide insight into how each predictor affects the response variable (in this case, \`mpg\`).
    -   By retrieving the coefficients at \`lambda.min\`, we can understand the relationship between the predictors and the target variable while accounting for the penalty imposed by regularization.
    -   This step helps in evaluating how much each feature contributes to the prediction with less risk of overfitting that may occur at lower lambda values (more complex models).

    ```{r}
    # Retrieve coefficients using the optimal lambda value
    ridge_coefficients <- coef(cv_model, s = "lambda.min")

    # Display the coefficients
    print(ridge_coefficients)
    ```

## Tasks for today

Following the bonus task in the previous tutorial - finding the optimal $\lambda$ in Ridge and Lasso regression:

-   The following code snippet prepares the airbnbsmall dataset to include only continuous variables (column name starting with "n\_") and convert X and Y into matrix form. Refers to online documentation or the lecture slides to run Ridge and Lasso regression. Try to interpret the result.

gmlnet() needs the variables as vector/matrix input. The following code snippets already take care of it for you.

```{r}
#install.packages("glmnet")
library(sozoekds)
library(glmnet) 
# Drop any rows the contain missing values
airbnbsmall <- na.omit(airbnbsmall)


# select columns with continuous variables starting with "n_"
n_cols <- grep("^n_", names(airbnbsmall), value = TRUE)
print(paste("Continuous variables in airbnbsmall:", n_cols))

# gmlnet() needs the variables as vector/matrix input
# X is a matrix that contains all variables from the regression of all continuous variables
X = model.matrix(price ~ ., airbnbsmall[, c("price", n_cols)])[, -1]
# y contains the endogenous variable
y = airbnbsmall$price

```

#### Ridge Regression

-   `cv.glmnet()` from the glmnet package takes care of doing k-fold cross-validation for you.
-   Ridge regression: alpha=0

```{r}
fit_ridge_cv = cv.glmnet(X, y, nfolds=10, alpha = 0)
plot(fit_ridge_cv)
```

```{r}
bestlambda <- fit_ridge_cv$lambda.min
cat("Best lambda value for Ridge regression is:", bestlambda, "\n")
coefficients <- coef(fit_ridge_cv, s = "lambda.min")
# Convert coefficients to a named numeric vector
coefficients_named <- round(as.numeric(coefficients), 2)
names(coefficients_named) <- rownames(coefficients)

# Print the named coefficients
cat("X variables and their coefficients: \n")
print(coefficients_named)
cat("Cross-validation error at lambda.min:", fit_ridge_cv$cvm[fit_ridge_cv$lambda == fit_ridge_cv$lambda.min], "\n")
```

We can see that Ridge regression does not remove variables whose weights are small. Instead, we can use Lasso regression for dimensionality reduction of input space.

#### Lasso regression

-   Lasso regression: alpha=1

```{r}
fit_lasso_cv = cv.glmnet(X, y, nfolds=10, alpha = 1)
plot(fit_lasso_cv)
```

```{r}
bestlambda <- fit_lasso_cv$lambda.min
cat("Best lambda value for Lasso regression is:", bestlambda, "\n")
coefficients <- coef(fit_lasso_cv, s = "lambda.min")
# Convert coefficients to a named numeric vector
coefficients_named <- round(as.numeric(coefficients), 2)
names(coefficients_named) <- rownames(coefficients)

# Print the named coefficients
cat("X variables and their coefficients: \n")
print(coefficients_named)
cat("Cross-validation error at lambda.min:", fit_lasso_cv$cvm[fit_lasso_cv$lambda == fit_lasso_cv$lambda.min], "\n")
```

### Questions:

1.  What are ridge and lasso regression, and how do they differ from standard linear regression?
2.  How would you interpret the coefficients of a lasso regression model if some of them are exactly zero?
3.  What socioeconomic variables might benefit from regularization through ridge or lasso regression?
4.  How can these regression techniques help in handling multicollinearity in socioeconomic datasets?
5.  When analyzing socioeconomic data, under what circumstances would you prefer lasso over ridge regression, or vice versa?

### Answer:

1.  <div>

    -   Ridge and lasso regression are types of regularized linear regression models designed to address overfitting and multicollinearity. Both add a penalty term to the linear regression cost function to shrink coefficients:

    <!-- -->

    -   Ridge regression penalizes the sum of the squared coefficients (L2-norm).

    -   Lasso regression penalizes the sum of the absolute values of the coefficients (L1-norm), which can set some coefficients to exactly zero, performing variable selection.

    </div>

2.  <div>

    -   Coefficients set to zero indicate variables that lasso determined to be non-influential in predicting the outcome. This simplifies the model by effectively excluding those variables.

    </div>

3.  <div>

    -   Variables with multicollinearity, such as education level, income, employment rate, and housing costs, often benefit.

    -   Regularization helps separate their individual effects on the outcome.

    -   Lasso regression helps feature selection and therefore reduce the dimensionality of features.

    </div>

4.  <div>

    -   By shrinking correlated variable coefficients, ridge regression prevents large swings in estimates.

    -   Lasso may select only one variable from a group of correlated variables, improving interpretability.

    </div>

5.  <div>

    -   Lasso is preferred when you wants to select only a subset of the predictors, as it performs variable selection.

    -   Ridge is better when all variables likely contribute to the outcome but may suffer from multicollinearity.

    -   If variables are highly correlated, lasso tends to select one variable from the group and ignore the others, which may lead to biased interpretations.

    </div>