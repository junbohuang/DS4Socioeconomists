---
title: "out-of-sample predictions, cross-validation and evaluation metrics - Session 10"
format: 
  html:
    self-contained: true
    theme: minty
editor: visual
---

# Recap

## Errors

Different kinds of errors help to asses model quality. You might remember them from statistics:

-   Someone has an illness and we are not able to detect that (false positive, type I error)

-   Someone is healthy and the model detects them as ill (false negative, type II error)

In statistical learning we use the confusion matrix as a summary of important features about the performance of the classification algorithm

![](images/conf_matrix_1.png)

![](images/conf_matrix_2.png)

Precision: $TP/P^*=TP/(FP+TP)$

The number of actual positive cases that the model identifies, relative to the total number of cases the model predicts as positive.

-   Actual positive cases: True positive (TP)

-   Positive predicted cases that are actually negative: false positive (FP)

-   all cases that are **predicted as positive** (true and false positives): P\*

Recall: $TP/P=TP/(TP+FN)$

The number of actual positive cases (sick individuals) that the model correctly identifies as positive, relative to the total number of actual positive cases.

-   Actual positive cases: True positive (TP)

-   negative predicted cases that are actually positive: false negative (FN)

-   all truly **positive** cases (true positives and false negatives): P

## Validation

::: callout-important
-   the validation set approach involves randomly dividing the available set of observations into two parts, a training set and a validation set or hold-out set.
-   The model is fit on the training set, and the fitted model is used to predict the responses for the observations in the validation set.
-   The resulting validation set error rate—typically assessed using MSE in the case of a quantitative response—provides an estimate of the test error rate.
:::

Different kinds of splits:

-   splitting just once at a certain percentage point

-   Leave-one-out cross-validation (LOOCV)

-   k-fold cross-validation (e.g. 10 splits)

## Application in R

1.  Creating training data set

```{r}
library(sozoekds)
set.seed(111) # to make the example reproducible, we set a fixed number

airbnbsmall <- airbnbsmall

#find out number of obervations, 70% in training dataset, 30% in test dataset
n_row <- as.numeric(nrow(airbnbsmall))
split_70 <- round(n_row *0.7, digits=0)

train <- sample(n_row, split_70)
```

2.  Setting up different models: We want to explain the price of an Airbnb listing by its number of bedrooms.

```{r}
library(dplyr) #used for the piping command ( %>% )
attach(airbnbsmall)

#1 linear regression
lm.fit <- lm(price ~ n_beds, data = airbnbsmall, subset = train)
rmse_1 <- mean((price - predict(lm.fit, airbnbsmall))[-train]^2) %>% 
  round(digits = 2)
print (paste("The RMSE for the linear regression is", rmse_1))

#2 
lm.fit2 <- lm(price ~ poly(n_beds, 2), data = airbnbsmall, subset = train)
rmse_2 <- mean((price - predict(lm.fit2, airbnbsmall))[-train]^2) %>% 
  round(digits = 2)
print (paste("The RMSE for the quadratic regression is", rmse_2))

#3 
lm.fit3 <- lm(price ~ poly(n_beds, 3), data = airbnbsmall, subset = train)
rmse_3 <- mean((price - predict(lm.fit3, airbnbsmall))[-train]^2) %>% 
  round(digits = 2)
print (paste("The RMSE for the cubic regression is", rmse_3))
```

**How can we interpret the results? Which of the models has the lowest test error rate?**

3.  Cross validation for polynomial regression models from the degrees 1 to 10 using 10 folds

    *This involves dividing the data into 10 parts, training the model on 9 parts and validating on 1 part, and repeating this process for all parts.*

```{r}
library(boot) #library needed for cross-validation
set.seed(111) # to make the example reproducible, we set a fixed number

cv.error.10 <- rep(0, 10)
for (i in 1:10) {
  glm.fit <- glm(price ~ poly(n_beds, i), data = airbnbsmall)
  cv.error.10[i] <- cv.glm(airbnbsmall, glm.fit, K = 10)$delta[1]
}
cv.error.10
```

Verbal code description: In a loop, we fit a GLM for each degree. The code computes the cross-validation error using 10 folds, and stores the resulting error in the vector. Finally, it outputs the average errors for each polynomial degree.

**How can we interpret the results? Which of the models has the lowest test error rate?**

## Tasks for today

## Task 1

Imagine you are working on a regression problem and have a dataset containing 500 samples. You decide to perform K-Fold Cross-Validation with K=5.

1.  Calculate the number of samples that will be included in each fold and explain what happens in each step during the cross-validation process.

2.  In each of the folds, you obtained the following Root Mean Squared Errors (RMSE):

-   Fold 1: RMSE = 2.5
-   Fold 2: RMSE = 3.0
-   Fold 3: RMSE = 2.8
-   Fold 4: RMSE = 2.9
-   Fold 5: RMSE = 3.1

Calculate the average RMSE across all folds.

3.  Discuss the significance of RMSE and what it implies if the average RMSE is high. What impact could this have on your model?

## Task 2

Use the airbnbsmall-dataset. Create an 80/20 split based on the code that you have seen in today's example.

We want to explain the price of a listing by the n_review_scores_rating

1.  Set up 5 different models using polynomial regression from degree 1 to 5. (hint: like in step 2 in today's example) and test them on an 80/20 split. Which of the models has the best out-of-sample performance / the lowest test set RMSE?

2.  Perform 5 fold cross validation on the dataset (using polynomial regression from degree 1 to 5) (hint: like in step 3 in today's example). Which of the models has the best out-of-sample performance / the lowest test set RMSE?

# Solutions

## **Task 1**

**Solution 1: Number of Samples per Fold**

If the dataset contains 500 samples and you are performing K-Fold Cross-Validation with K=5, you will split the data into 5 equal-sized folds.

**Calculation:** - Number of samples per fold = Total number of samples / K - Number of samples per fold = 500 / 5 = 100

Each fold will contain 100 samples.

**Explanation:** In each step of the cross-validation, one fold is used as the test set while the remaining 4 folds serve as the training set. This process is repeated for all 5 folds, ensuring that every sample in the dataset is used exactly once in the test set.

**Solution 2: Calculate Average RMSE**

The RMSE values for the individual folds are: - Fold 1: RMSE = 2.5 - Fold 2: RMSE = 3.0 - Fold 3: RMSE = 2.8 - Fold 4: RMSE = 2.9 - Fold 5: RMSE = 3.1

**Calculation:** To calculate the average RMSE, add the RMSE values and divide by the number of folds.

Average RMSE = (2.5 + 3.0 + 2.8 + 2.9 + 3.1) / 5 = (14.3) / 5 = 2.86

Thus, the average RMSE across all folds is 2.86.

**Solution 3: Significance of RMSE**

The RMSE is a measure of the average deviation of predictions from the actual values and indicates how well the model captures the underlying patterns in the data. A low RMSE indicates that the model's predictions are close to the actual values. If the average RMSE is high, it could imply several things:

-   The model is struggling to capture the underlying patterns in the data.
-   It may indicate the selection of an inadequate model or insufficient features.
-   A high RMSE could also suggest that the model might be overfitting or that there are issues with the quality of the data (e.g., outliers or non-representative data).

In this case, it may be necessary to revise the model, experiment with different modelling approaches, or take additional data preprocessing steps to improve model performance.

## Task 2

1.  Create the split

```{r}
library(sozoekds)
set.seed(111) # to make the example reproducible, we set a fixed number

airbnbsmall <- airbnbsmall

#find out number of obervations, 80% in training dataset, 20% in test dataset
n_row <- as.numeric(nrow(airbnbsmall))
split_80 <- round(n_row *0.8, digits=0)

train <- sample(n_row, split_80)
```

2.  Calculate RMSE for GLMs with degree 1 to 5 based on the training/test split of 80/20

```{r}
library(dplyr)
attach(airbnbsmall)

#1 linear regression
lm.fit <- lm(price ~ n_review_scores_rating, data = airbnbsmall, subset = train)
rmse_1 <- mean((price - predict(lm.fit, airbnbsmall))[-train]^2) %>% 
  round(digits = 2)
print (paste("The RMSE for the linear regression is", rmse_1))

#2 
lm.fit2 <- lm(price ~ poly(n_review_scores_rating, 2), data = airbnbsmall, subset = train)
rmse_2 <- mean((price - predict(lm.fit2, airbnbsmall))[-train]^2) %>% 
  round(digits = 2)
print (paste("The RMSE for the quadratic regression is", rmse_2))

#3 
lm.fit3 <- lm(price ~ poly(n_review_scores_rating, 3), data = airbnbsmall, subset = train)
rmse_3 <- mean((price - predict(lm.fit3, airbnbsmall))[-train]^2) %>% 
  round(digits = 2)
print (paste("The RMSE for the cubic regression is", rmse_3))

#4 
lm.fit4 <- lm(price ~ poly(n_review_scores_rating, 4), data = airbnbsmall, subset = train)
rmse_4 <- mean((price - predict(lm.fit4, airbnbsmall))[-train]^2) %>% 
  round(digits = 2)
print (paste("The RMSE for the quartic regression is", rmse_4))

#5 
lm.fit5 <- lm(price ~ poly(n_review_scores_rating, 5), data = airbnbsmall, subset = train)
rmse_5 <- mean((price - predict(lm.fit5, airbnbsmall))[-train]^2) %>% 
  round(digits = 2)
print (paste("The RMSE for the quintic regression is", rmse_5))
```

Interpretation: A lower RMSE represents a better predictive performance of the model.

The quadratic regression has the lowest RMSE of all the models, therefore the highest predictive power. After the second degree the RMSE rises until the fifth degree that is lower than the fourth. This indicates that the degree 5 polynomial regression has a higher predictive power than the one for degree 4 which could be due to over fitting of the model. And still the value is higher than for degree 2.

```{r}
library(boot) #library needed for cross-validation
set.seed(111) # to make the example reproducible, we set a fixed number

cv.error.5 <- rep(0, 5)
for (i in 1:5) {
  glm.fit <- glm(price ~ poly(n_review_scores_rating, i), data = airbnbsmall)
  cv.error.5[i] <- cv.glm(airbnbsmall, glm.fit, K = 5)$delta[1]
}
cv.error.5
```

Interpretation: The 5 values represent the average RMSE over all 5 folds for each polynomial degree. The average RMSE for the second degree is lower than for all other degrees. Therefore a quadratic regression has the highest predictive power.
