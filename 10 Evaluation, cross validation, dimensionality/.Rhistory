print(y$train_data)
print(train_data[y])
print(y)
model <- lm(y ~ ., data = train_data # use "y_train" as outcome variable; use the training data by creating a dataframe from y_train and x_train
summary(model)
model <- lm(y ~ ., data = train_data) # use "y_train" as outcome variable; use the training data by creating a dataframe from y_train and x_train
model <- lm(y ~ xvar, data = train_data) # use "y_train" as outcome variable; use the training data by creating a dataframe from y_train and x_train
model <- lm(y ~ d_breakfast+n_accommodates+n_number_of_reviews, data = train_data) # use "y_train" as outcome variable; use the training data by creating a dataframe from y_train and x_train
model <- lm(price ~ d_breakfast+n_accommodates+n_number_of_reviews, data = train_data) # use "y_train" as outcome variable; use the training data by creating a dataframe from y_train and x_train
model <- lm(price ~ xvar, data = train_data) # use "y_train" as outcome variable; use the training data by creating a dataframe from y_train and x_train
rm(list=ls())
# preparation
library(tidyverse)
library(dplyr)
## welche libraries noch??
library(caret) # to split the sample
# load data
airbnb_data <- readRDS("Z:/wiso/SozOek_EmpWifo/Gemeinsam/L_Projekte/Data_Science_Kurs_WiSe_2023_24/Material/Datasets/airbnb/final/airbnb_data_small.rds")
#------------------------------------------------
##################################
### GROUP 1: LINEAR REGRESSION ###
##################################
# Define the x-variables and output variable
xvar <- c("d_breakfast", "n_accommodates", "n_number_of_reviews") # creates a column w/ those 3 variables; "stored" in object "xvar"; take exogenous variables from your model
x <- airbnb_data[, xvar] # defines x as the variables in xvar
y <- airbnb_data$price # defines y as "price" in the airbnb dataset
# Split the data into training and testing sets (70% training, 30% testing)
set.seed(123)  # for reproducibility; to get the same random split each time you run your code
# creates the training data
trainIndex <- createDataPartition(y, p = 0.7, # percentage of data going to training
list = FALSE,
times = 1) # only 1 split
train_data <- airbnb_data[trainIndex, ]
test_data <- airbnb_data[-trainIndex, ]
model <- lm(price ~ xvar, data = train_data) # use "y_train" as outcome variable; use the training data by creating a dataframe from y_train and x_train
model <- lm(price ~ d_breakfast+n_accommodates+n_number_of_reviews, data = train_data) # use "y_train" as outcome variable; use the training data by creating a dataframe from y_train and x_train
summary(model)
out_predictions <- predict(model, newdata = test_data))
out_predictions <- predict(model, newdata = test_data)
out_error <- (out_predictions - y$test_data) # calculate the error (predicted values-observed values in the test set)
price_test <- price$test_data
price$test_data
price$train_data
print(price$train_data)
print(train_data)
print(price$train_data)
print(price)
out_error <- (out_predictions - y$test_data) # calculate the error (predicted values-observed values in the test set)
y$test_data
print(y$test_data)
y_test <- y[test_data]
rm(list=ls())
# preparation
library(tidyverse)
library(dplyr)
## welche libraries noch??
library(caret) # to split the sample
# load data
airbnb_data <- readRDS("Z:/wiso/SozOek_EmpWifo/Gemeinsam/L_Projekte/Data_Science_Kurs_WiSe_2023_24/Material/Datasets/airbnb/final/airbnb_data_small.rds")
#------------------------------------------------
##################################
### GROUP 1: LINEAR REGRESSION ###
##################################
# Define the x-variables and output variable
xvar <- c("d_breakfast", "n_accommodates", "n_number_of_reviews") # creates a column w/ those 3 variables; "stored" in object "xvar"; take exogenous variables from your model
x <- airbnb_data[, xvar] # defines x as the variables in xvar
y <- airbnb_data$price # defines y as "price" in the airbnb dataset
# Split the data into training and testing sets (70% training, 30% testing)
set.seed(123)  # for reproducibility; to get the same random split each time you run your code
# creates the training data
trainIndex <- createDataPartition(y, p = 0.7, # percentage of data going to training
list = FALSE,
times = 1) # only 1 split
x_train <- x[trainIndex, ] # stores the required x-variables from the training set
x_test <- x[-trainIndex, ] # stores the required x-variables from the test set
y_train <- y[trainIndex] # stores the required y-variables from the training set
y_test <- y[-trainIndex] # # stores the required y-variables from the test set
# Train a linear regression model on the training data
model <- lm(y_train ~ ., data = data.frame(cbind(y_train, x_train))) # use "y_train" as outcome variable; use the training data by creating a dataframe from y_train and x_train
summary(model)
out_predictions <- predict(model, newdata =x_test)
rm(list=ls())
# preparation
library(tidyverse)
library(dplyr)
## welche libraries noch??
library(caret) # to split the sample
# load data
airbnb_data <- readRDS("Z:/wiso/SozOek_EmpWifo/Gemeinsam/L_Projekte/Data_Science_Kurs_WiSe_2023_24/Material/Datasets/airbnb/final/airbnb_data_small.rds")
#------------------------------------------------
##################################
### GROUP 1: LINEAR REGRESSION ###
##################################
# Define the x-variables and output variable
xvar <- c("d_breakfast", "n_accommodates", "n_number_of_reviews") # creates a column w/ those 3 variables; "stored" in object "xvar"; take exogenous variables from your model
x <- airbnb_data[, xvar] # defines x as the variables in xvar
y <- airbnb_data$price # defines y as "price" in the airbnb dataset
# Split the data into training and testing sets (70% training, 30% testing)
set.seed(123)  # for reproducibility; to get the same random split each time you run your code
# creates the training data
trainIndex <- createDataPartition(y, p = 0.7, # percentage of data going to training
list = FALSE,
times = 1) # only 1 split
x_train <- x[trainIndex, ]
x_test <- x[-trainIndex, ]
y_train <- y[trainIndex]
y_test <- y[-trainIndex]
# Train a linear regression model on the training data
model <- lm(y_train ~ ., data = data.frame(cbind(y_train, x_train))) # use "y_train" as outcome variable; use the training data by creating a dataframe from y_train and x_train
summary(model)
# Make predictions on the test set using the trained model "model"
out_predictions <- predict(model, newdata = x_test)
rm(list=ls())
# preparation
library(tidyverse)
library(dplyr)
## welche libraries noch??
library(caret) # to split the sample
# load data
airbnb_data <- readRDS("Z:/wiso/SozOek_EmpWifo/Gemeinsam/L_Projekte/Data_Science_Kurs_WiSe_2023_24/Material/Datasets/airbnb/final/airbnb_data_small.rds")
#------------------------------------------------
##################################
### GROUP 1: LINEAR REGRESSION ###
##################################
# Define the x-variables and output variable
xvar <- c("d_breakfast", "n_accommodates", "n_number_of_reviews") # creates a column w/ those 3 variables; "stored" in object "xvar"; take exogenous variables from your model
x <- airbnb_data[, xvar] # defines x as the variables in xvar
y <- airbnb_data$price # defines y as "price" in the airbnb dataset
# Split the data into training and testing sets (70% training, 30% testing)
set.seed(123)  # for reproducibility; to get the same random split each time you run your code
# creates the training data
trainIndex <- createDataPartition(y, p = 0.7, # percentage of data going to training
list = FALSE,
times = 1) # only 1 split
x_train <- x[trainIndex, ]
x_test <- x[-trainIndex, ]
y_train <- y[trainIndex]
y_test <- y[-trainIndex]
# Train a linear regression model on the training data
model <- lm(y_train ~ ., data = data.frame(cbind(y_train, x_train))) # use "y_train" as outcome variable; use the training data by creating a dataframe from y_train and x_train
summary(model)
# Make predictions on the test set using the trained model "model"
out_predictions <- predict(model, newdata = x_test)
out_error <- (out_predictions - y_test) # calculate the error (predicted values-observed values in the test set)
sqrt(mean(out_error^2)) # RMSE
out_predictions <- predict(model, newdata = data.frame(cbind(y_test, x_test)))
out_error <- (out_predictions - y_test) # calculate the error (predicted values-observed values in the test set)
sqrt(mean(out_error^2))
# define the x-variables
xvar <- c("n_number_of_reviews", "price", "n_days_since") # creates a column w/ those 3 variables; "stored" in object "xvar"; take exogenous variables from your model
x <- airbnb_data[, xvar] # defines x as the variables in xvar
# define output variable
airbnb_data <- airbnb_data %>%
mutate(
high_rating = ifelse(airbnb_data$n_review_scores_rating>94, 1, 0)
)
y <- airbnb_data$high_rating # defines y as "high_rating" in the airbnb dataset
# Split the data into training and testing sets (70% training, 30% testing)
set.seed(123)  # for reproducibility; to get the same random split each time you run your code
# creates the training data
trainIndex <- createDataPartition(y, p = 0.7, # percentage of data going to training
list = FALSE,
times = 1) # only 1 split
x_train <- x[trainIndex, ]
x_test <- x[-trainIndex, ]
y_train <- y[trainIndex]
y_test <- y[-trainIndex]
# Train a logistic regression model on the training data
model <- glm(y_train ~ ., family=binomial(link="logit"), data = data.frame(cbind(y_train, x_train))) # use "y_train" as outcome variable; use the training data by creating a dataframe from y_train and x_train
summary(model)
# Make predictions on the test set using the trained model "model"
out_predictions <- predict(model, newdata = x_test, type = "response") # need to specify the type of dependent variable to make predictions on the DV's original scale
out_error <- (out_predictions - y_test) # calculate the error (predicted values-observed values in the test set)
sqrt(mean(out_error^2))
p_class <- factor(ifelse(out_predictions > 0.5, 1, 0), levels = c(0, 1), labels = c("Negative_Class", "Positive_Class"))
# check if same levels
unique(levels(p_class))
unique(levels(y_test))
# no, therefore:
# levels in predicted values and observed values need to be the same
# therefore transform y_test variables into factor w/ same labels as p_class
y_test_f <- factor(y_test, levels = c(0, 1), labels = c("Negative_Class", "Positive_Class"))
# check true positives & negatives as well as false positive & negatives
table(p_class, y_test_f)
n <- 1863+1733+1410+1184
print((1863+1733)/n)
rm(list=ls())
# preparation
library(tidyverse)
library(dplyr)
library(caret) # for splitting, CV
# load data
airbnb_data <- readRDS("Z:/wiso/SozOek_EmpWifo/Gemeinsam/L_Projekte/Data_Science_Kurs_WiSe_2023_24/Material/Datasets/airbnb/final/airbnb_data_small.rds")
airbnb_data <- airbnb_data %>%
mutate(
high_rating = ifelse(airbnb_data$n_review_scores_rating>94, 1, 0)
)
# specify y and x-variables of your model
xvar <- c("n_number_of_reviews", "price", "n_days_since") # creates a column w/ those 3 variables; "stored" in object "xvar"; take exogenous variables from your model
x <- airbnb_data[, xvar] # defines x as the variables in xvar
y <- airbnb_data$high_rating # defines y as "high_rating" in the airbnb dataset
print(y)
print(x)
model <- train(
y ~ x, # specifies the model w/ y as endogenous, x as exogenous variables
airbnb_data, # we use the airbnb_data
method = "logreg",# we use logistic regression
trControl = trainControl(
method = "cv", # we perform cross-validation
number = 10, # w/ k=10
verboseIter = TRUE
)
)
model <- train(
high_rating ~ n_number_of_reviews+price+n_days_since, # specifies the model w/ "high_rating" as endogenous, the 3 variables as exogenous variables
airbnb_data, # we use the airbnb_data
method = "logreg",# we use logistic regression
trControl = trainControl(
method = "cv", # we perform cross-validation
number = 10, # w/ k=10
verboseIter = TRUE
)
)
model <- train(
high_rating ~ n_number_of_reviews+price+n_days_since, # specifies the model w/ "high_rating" as endogenous, the 3 variables as exogenous variables
airbnb_data, # we use the airbnb_data
method = "logreg",# we use logistic regression
trControl = trainControl(
method = "cv", # we perform cross-validation
number = 10, # w/ k=10
verboseIter = TRUE
)
)
rm(list=ls())
library(tidyverse)
library(dplyr)
library(caret) # for splitting, CV
library(LogicReg)
# load data
airbnb_data <- readRDS("Z:/wiso/SozOek_EmpWifo/Gemeinsam/L_Projekte/Data_Science_Kurs_WiSe_2023_24/Material/Datasets/airbnb/final/airbnb_data_small.rds")
airbnb_data <- airbnb_data %>%
mutate(
high_rating = ifelse(airbnb_data$n_review_scores_rating>94, 1, 0)
)
print(high_rating)
print(airbnb_data$high_rating)
model <- train(
high_rating ~ n_number_of_reviews+price+n_days_since, # specifies the model w/ "high_rating" as endogenous, the 3 variables as exogenous variables
airbnb_data, # we use the airbnb_data
method = "logreg",# we use logistic regression
trControl = trainControl(
method = "cv", # we perform cross-validation
number = 10, # w/ k=10
verboseIter = TRUE
)
)
model <- lm(high_rating ~ n_number_of_reviews+price+n_days_since)
rm(list=ls())
# load packages/libraries
# install.packages("remotes") # hashtagged because already installed
library(remotes)
remotes::install_gitlab("BAQ6370/sozoekds", host="gitlab.rrz.uni-hamburg.de")
library(sozoekds)
library(tidyverse)
library(dplyr)
airbnb_data <- airbnbsmall # store data as "airbnb_data"
# create dummy variable for "high_rating" again
airbnb_data$high_rating = ifelse(airbnb_data$n_review_scores_rating>94, 1, 0) # adds the variable "high_rating" to the "airbnb_data" dataset; "ifelse" function to c
# run logistic regression
logreg <-glm(high_rating~n_number_of_reviews+price+d_gym,
family=binomial(link="logit"),data=airbnb_data) # glm + family=binomial(link=logit) for logistic regressions
summary(logreg) # show results
mod2<-lm(price~d_breakfast+n_accommodates+n_number_of_reviews,data=airbnb_data) # create the object "mod2" that contains the linear regression model; "lm" function for linear regression; left to "~" stands the endogenous variable, right to "~" the exogenous variables; "airbnb_data" is the dataset used for analysis
summary(mod2)
rm(list=ls())
# load packages/libraries
# install.packages("remotes") # hashtagged because already installed
library(remotes)
remotes::install_gitlab("BAQ6370/sozoekds", host="gitlab.rrz.uni-hamburg.de")
library(sozoekds)
library(tidyverse)
library(dplyr)
library(caret) # for splitting, CV
airbnb_data <- airbnbsmall # store data as "airbnb_data"
xvar <- c("d_breakfast", "n_accommodates", "n_number_of_reviews") # creates a column w/ those 3 variables; "stored" in object "xvar"; take exogenous variables from your model
x <- airbnb_data[, xvar] # defines x as the variables in xvar
y <- airbnb_data$price # defines y as "price" in the airbnb dataset
# Split the data into training and testing sets (70% training, 30% testing)
set.seed(123)  # for reproducibility; to get the same random split each time you run your code
# creates the training data
trainIndex <- createDataPartition(y, p = 0.7, # percentage of data going to training
list = FALSE,
times = 1) # only 1 split
x_train <- x[trainIndex, ]
x_test <- x[-trainIndex, ]
y_train <- y[trainIndex]
y_test <- y[-trainIndex]
model <- lm(y_train ~ ., data = data.frame(cbind(y_train, x_train))) # use "y_train" as outcome variable; use the training data by creating a dataframe from y_train and x_train
summary(model)
out_predictions <- predict(model, newdata = data.frame(x_test))
residuals <- (y_test-out_predictions) # calculate the error (predicted values-observed values in the test set)
sqrt(mean(residuals^2))
set.seed(123) # for reproducability
# use train & trainControl to perform cross-validation
model10 <- train(
price ~ n_number_of_reviews+n_accommodates+d_breakfast, # specifies the model w/ "price" as endogenous, the 3 variables as exogenous variables
data = airbnb_data, # we use the airbnb_data
method = "lm", # we use linear regression
trControl = trainControl(
method = "cv", # we perform cross-validation
number = 10, # w/ k=10
verboseIter = TRUE
)
)
model10 # shows results, i.e. RMSE = 59.4
p_class <- factor(ifelse(out_predictions > 0.5, 1, 0), levels = c(0, 1), labels = c("Negative_Class", "Positive_Class"))
y_test_f <- factor(y_test, levels = c(0, 1), labels = c("Negative_Class", "Positive_Class"))
confusionMatrix(p_class, y_test_f)
xvar <- c("n_number_of_reviews", "price", "d_gym") # creates a column w/ those 3 variables; "stored" in object "xvar"; take exogenous variables from your model
x <- airbnb_data[, xvar] # defines x as the variables in xvar
# define output variable
airbnb_data <- airbnb_data %>%
mutate(
high_rating = ifelse(airbnb_data$n_review_scores_rating>94, 1, 0)
)
y <- airbnb_data$high_rating # defines y as "high_rating" in the airbnb dataset
# Split the data into training and testing sets (70% training, 30% testing)
set.seed(123)  # for reproducibility; to get the same random split each time you run your code
# creates the training data
trainIndex <- createDataPartition(y, p = 0.7, # percentage of data going to training
list = FALSE,
times = 1) # only 1 split
x_train <- x[trainIndex, ]
x_test <- x[-trainIndex, ]
y_train <- y[trainIndex]
y_test <- y[-trainIndex]
# Train a logistic regression model on the training data
model <- glm(y_train ~ ., family=binomial(link="logit"), data = data.frame(cbind(y_train, x_train))) # use "y_train" as outcome variable; use the training data by creating a dataframe from y_train and x_train
summary(model)
# Make predictions on the test set using the trained model "model"
out_predictions <- predict(model, newdata = data.frame(cbind(y_test, x_test)), type = "response") # need to specify the type of dependent variable to make predictions on the DV's original scale
### RSME### --> maybe to be left out
residuals <- (y_test-out_predictions) # calculate the error (predicted values-observed values in the test set)
sqrt(mean(residuals^2))
p_class <- factor(ifelse(out_predictions > 0.5, 1, 0), levels = c(0, 1), labels = c("Negative_Class", "Positive_Class"))
# check if same levels
unique(levels(p_class))
unique(levels(y_test))
# no, therefore:
# levels in predicted values and observed values need to be the same
# therefore transform y_test variables into factor w/ same labels as p_class
y_test_f <- factor(y_test, levels = c(0, 1), labels = c("Negative_Class", "Positive_Class"))
table(p_class, y_test_f)
confusionMatrix(p_class, y_test_f)
rm(list=ls())
# load packages/libraries
# install.packages("remotes") # hashtagged because already installed
library(remotes)
remotes::install_gitlab("BAQ6370/sozoekds", host="gitlab.rrz.uni-hamburg.de")
library(sozoekds)
library(tidyverse)
library(dplyr)
library(caret) # for splitting, CV
airbnb_data <- airbnbsmall # store data as "airbnb_data"
##################
# 1. split data into training and test set
# Define the x-variables of your model
xvar <- c("n_number_of_reviews", "price", "d_gym") # creates a column w/ those 3 variables; "stored" in object "xvar"
x <- airbnb_data[, xvar] # defines x as the variables in xvar
# Define the output variable of your model
airbnb_data$high_rating = ifelse(airbnb_data$n_review_scores_rating>94, 1, 0) # create high rating again
y <- airbnb_data$high_rating # defines y as "high_rating" in the airbnb dataset
# Split the data into training and testing sets (70% training, 30% testing)
set.seed(123)  # for reproducibility; to get the same random split each time you run your code
# creates the training data
trainIndex <- createDataPartition(y, p = 0.7, # percentage of data going to training
list = FALSE,
times = 1) # only 1 split
x_train <- x[trainIndex, ]
x_test <- x[-trainIndex, ]
y_train <- y[trainIndex]
y_test <- y[-trainIndex]
model <- lm(y_train ~ ., family=binomial(link="logit"), data = data.frame(cbind(y_train, x_train))) # use "y_train" as outcome variable; use the training data by creating a dataframe from y_train and x_train
summary(model)
logreg <-glm(high_rating~n_number_of_reviews+price+d_gym,
family=binomial(link="logit"),data=airbnb_data) # glm + family=binomial(link=logit) for logistic regressions
summary(logreg)
model <- glm(y_train ~ ., family=binomial(link="logit"), data = data.frame(cbind(y_train, x_train))) # use "y_train" as outcome variable; use the training data by creating a dataframe from y_train and x_train
summary(model)
model <- glm(y_train ~ ., family=binomial, data = data.frame(cbind(y_train, x_train))) # use "y_train" as outcome variable; use the training data by creating a dataframe from y_train and x_train
summary(model)
model <- glm(y_train ~ ., family=binomial(link="logit"), data = data.frame(cbind(y_train, x_train))) # use "y_train" as outcome variable; use the training data by creating a dataframe from y_train and x_train
summary(model)
out_predictions <- predict(model, newdata = data.frame(x_test))
residuals <- (y_test-out_predictions) # calculate the error (predicted values-observed values in the test set)
sqrt(mean(residuals^2))
set.seed(123) # for reproducability
# use train & trainControl to perform cross-validation
model5 <- train(
model,
data = airbnb_data, # we use the airbnb_data
method = "glm", # we use logistic regression
trControl = trainControl(
method = "cv", # we perform cross-validation
number = 5, # w/ k=10
verboseIter = TRUE
)
)
set.seed(123) # for reproducability
# use train & trainControl to perform cross-validation
model5 <- train(
high_rating ~ n_number_of_reviews+price+d_gym, # specifies the model
data = airbnb_data, # we use the airbnb_data
method = "glm", # we use linear regression
trControl = trainControl(
method = "cv", # we perform cross-validation
number = 5, # w/ k=10
verboseIter = TRUE
)
)
model5 <- train(
high_rating ~ n_number_of_reviews+price+d_gym, # specifies the model
data = airbnb_data, # we use the airbnb_data
method = "glm", # we use logistic regression
family=binomial, # we use logistic regression
trControl = trainControl(
method = "cv", # we perform cross-validation
number = 5, # w/ k=10
verboseIter = TRUE
)
)
model5
cross <- trainControl(method="cv",
number=5,
verboseIter = TRUE)
modelcross <- train(model, data=data.frame(cbind(y_train, x_train)),
method="glm",
family=binomial,
trControl=cross) # applies our cross-validation process as specified above
modelcross <- train(y_train ~ ., data=data.frame(cbind(y_train, x_train)),
method="glm",
family=binomial,
trControl=cross) # applies our cross-validation process as specified above
modelcross
set.seed(123) # for reproducability
# specify cross-validation & number of folds
cross <- trainControl(method="cv", # cross-validation
number=10, # k=10
verboseIter = TRUE)
# specify model to be estimated using training data & apply cv
modelcross <- train(y_train ~ ., data=data.frame(cbind(y_train, x_train)),
method="glm",
family=binomial,
trControl=cross) # applies our cross-validation process as specified above
modelcross # prints RMSE 0.4941984
set.seed(123) # for reproducability
# specify cross-validation & number of folds
cross <- trainControl(method="cv", # cross-validation
number=50, # k=10
verboseIter = TRUE)
# specify model to be estimated using training data & apply cv
modelcross <- train(y_train ~ ., data=data.frame(cbind(y_train, x_train)),
method="glm",
family=binomial,
trControl=cross) # applies our cross-validation process as specified above
modelcross # prints RMSE 0.4946834
p_class <- factor(ifelse(out_predictions > 0.5, 1, 0), levels = c(0, 1), labels = c("Negative_Class", "Positive_Class")) # creates a factor being 1 when predicted probability >0.5, 0 when <0.5; labelled as "Positive_Class" and "Negative_class"
confusionMatrix(p_class, y_test_f)
y_test_f <- factor(y_test, levels = c(0, 1), labels = c("Negative_Class", "Positive_Class"))
confusionMatrix(p_class, y_test)
unique(levels(p_class))
unique(levels(y_test))
rm(list=ls())
# load packages/libraries
# install.packages("remotes") # hashtagged because already installed
library(remotes)
remotes::install_gitlab("BAQ6370/sozoekds", host="gitlab.rrz.uni-hamburg.de")
library(sozoekds)
library(tidyverse)
library(dplyr)
library(caret) # for splitting, CV
airbnb_data <- airbnbsmall # store data as "airbnb_data"
##################
# 1. split data into training and test set
# Define the x-variables of your model
xvar <- c("n_number_of_reviews", "price", "d_gym") # creates a column w/ those 3 variables; "stored" in object "xvar"
x <- airbnb_data[, xvar] # defines x as the variables in xvar
# Define the output variable of your model
airbnb_data$high_rating = ifelse(airbnb_data$n_review_scores_rating>94, 1, 0) # create high rating again
y <- airbnb_data$high_rating # defines y as "high_rating" in the airbnb dataset
# Split the data into training and testing sets (70% training, 30% testing)
set.seed(123)  # for reproducibility; to get the same random split each time you run your code
# creates the training data
trainIndex <- createDataPartition(y, p = 0.7, # percentage of data going to training
list = FALSE,
times = 1) # only 1 split
x_train <- x[trainIndex, ]
x_test <- x[-trainIndex, ]
y_train <- y[trainIndex]
y_test <- y[-trainIndex]
# 2. Train your model on the training data
model <- glm(y_train ~ ., family=binomial(link="logit"), data = data.frame(cbind(y_train, x_train))) # use "y_train" as outcome variable; use the training data by creating a dataframe from y_train and x_train
summary(model)
# 3. Make predictions on the test set using the trained model
out_predictions <- predict(model, newdata = data.frame(x_test))
p_class <- factor(ifelse(out_predictions > 0.5, 1, 0), levels = c(0, 1), labels = c("Negative_Class", "Positive_Class")) # creates a factor being 1 when predicted probability >0.5, 0 when <0.5; labelled as "Positive_Class" and "Negative_class"
# levels in predicted values and observed values need to be the same, but are different:
unique(levels(p_class)) # prints "Negative_Class" "Positive_Class"
unique(levels(y_test)) # prints NULL
# therefore transform y_test variables into factor w/ same labels as p_class
y_test_f <- factor(y_test, levels = c(1, 0), labels = c("Positive_Class", "Negative_Class"))
confusionMatrix(p_class, y_test_f, mode = "prec_recall" ) # prints confusion matrix
