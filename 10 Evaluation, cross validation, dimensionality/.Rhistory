lm.fit2 <- glm(price ~ poly(n_beds, 2), data = airbnbsmall, subset = train)
rmse_2 <- mean((price - predict(lm.fit2, airbnbsmall))[-train]^2) %>%
round(digits = 2)
print (paste("The RMSE for the quadratic regression is", rmse_2))
#3
lm.fit3 <- glm(price ~ poly(n_beds, 3), data = airbnbsmall, subset = train)
rmse_3 <- mean((price - predict(lm.fit3, airbnbsmall))[-train]^2) %>%
round(digits = 2)
print (paste("The RMSE for the cubic regression is", rmse_3))
cross_validation_fits_placeholder <- numeric(10) # Initialize placeholder for results
# Perform cross-validation
for (i in 1:10) {
# Dynamically create the formula
formula <- as.formula(paste("price ~ poly(n_beds,", i, ")"))
# Fit the model using glm
model <- glm(price ~ poly(n_beds, i), data = airbnbsmall)
# Perform cross-validation
cv_result <- cv.glm(airbnbsmall, model, K = 10)
# Store the cross-validation error (delta[1])
cross_validation_fits_placeholder[i] <- cv_result$delta[1]
library(sozoekds)
library(dplyr) #used for the piping command ( %>% )
library(boot) #library needed for cross-validation
set.seed(111) # to make the example reproducible, we set a fixed number
airbnbsmall <- airbnbsmall
#find out number of obervations, 70% in training dataset, 30% in test dataset
n_row <- as.numeric(nrow(airbnbsmall))
split_70 <- round(n_row *0.7, digits=0)
train <- sample(n_row, split_70)
attach(airbnbsmall)
#1 linear regression
lm.fit <- glm(price ~ n_beds, data = airbnbsmall, subset = train)
rmse_1 <- mean((price - predict(lm.fit, airbnbsmall))[-train]^2) %>%
round(digits = 2)
print (paste("The RMSE for the linear regression is", rmse_1))
#2
lm.fit2 <- glm(price ~ poly(n_beds, 2), data = airbnbsmall, subset = train)
rmse_2 <- mean((price - predict(lm.fit2, airbnbsmall))[-train]^2) %>%
round(digits = 2)
print (paste("The RMSE for the quadratic regression is", rmse_2))
#3
lm.fit3 <- glm(price ~ poly(n_beds, 3), data = airbnbsmall, subset = train)
rmse_3 <- mean((price - predict(lm.fit3, airbnbsmall))[-train]^2) %>%
round(digits = 2)
print (paste("The RMSE for the cubic regression is", rmse_3))
cross_validation_fits_placeholder <- numeric(10) # Initialize placeholder for results
# Perform cross-validation
for (i in 1:10) {
# Dynamically create the formula
formula <- as.formula(paste("price ~ poly(n_beds,", i, ")"))
# Fit the model using glm
model <- glm(price ~ poly(n_beds, i), data = airbnbsmall)
# Perform cross-validation
cv_result <- cv.glm(airbnbsmall, model, K = 10)
# Store the cross-validation error (delta[1])
cross_validation_fits_placeholder[i] <- cv_result$delta[1]
cross_validation_fits_placeholder <- numeric(10) # Initialize placeholder for results
# Perform cross-validation
for (i in 1:10) {
# Dynamically create the formula
formula <- as.formula(paste("price ~ poly(n_beds,", i, ")"))
# Fit the model using glm
model <- glm(price ~ poly(n_beds, i), data = airbnbsmall)
# Perform cross-validation
cv_result <- cv.glm(airbnbsmall, model, K = 10)
# Store the cross-validation error (delta[1])
cross_validation_fits_placeholder[i] <- cv_result$delta[1]
}
# Display results
print(cross_validation_fits_placeholder)
library(sozoekds)
library(dplyr) #used for the piping command ( %>% )
library(boot) #library needed for cross-validation
set.seed(111) # to make the example reproducible, we set a fixed number
airbnbsmall <- airbnbsmall
#find out number of obervations, 70% in training dataset, 30% in test dataset
n_row <- as.numeric(nrow(airbnbsmall))
split_70 <- round(n_row *0.7, digits=0)
train <- sample(n_row, split_70)
attach(airbnbsmall)
#1 linear regression
lm.fit <- glm(price ~ n_beds, data = airbnbsmall, subset = train)
rmse_1 <- mean((price - predict(lm.fit, airbnbsmall))[-train]^2) %>%
round(digits = 2)
print (paste("The RMSE for the linear regression is", rmse_1))
#2
lm.fit2 <- glm(price ~ poly(n_beds, 2), data = airbnbsmall, subset = train)
rmse_2 <- mean((price - predict(lm.fit2, airbnbsmall))[-train]^2) %>%
round(digits = 2)
print (paste("The RMSE for the quadratic regression is", rmse_2))
#3
lm.fit3 <- glm(price ~ poly(n_beds, 3), data = airbnbsmall, subset = train)
rmse_3 <- mean((price - predict(lm.fit3, airbnbsmall))[-train]^2) %>%
round(digits = 2)
print (paste("The RMSE for the cubic regression is", rmse_3))
cross_validation_fits_placeholder <- numeric(10) # Initialize placeholder for results
# Perform cross-validation
for (i in 1:10) {
# Dynamically create the formula
formula <- as.formula(paste("price ~ poly(n_beds,", i, ")"))
# Fit the model using glm
model <- glm(price ~ poly(n_beds, i), data = airbnbsmall)
# Perform cross-validation
cv_result <- cv.glm(airbnbsmall, model, K = 10)
# Store the cross-validation error (delta[1])
cross_validation_fits_placeholder[i] <- cv_result$delta[1]
cross_validation_fits_placeholder <- numeric(10) # Initialize placeholder for results
# Perform cross-validation
for (i in 1:10) {
# Dynamically create the formula
formula <- as.formula(paste("price ~ poly(n_beds,", i, ")"))
# Fit the model using glm
model <- glm(price ~ poly(n_beds, i), data = airbnbsmall)
# Perform cross-validation
cv_result <- cv.glm(airbnbsmall, model, K = 10)
# Store the cross-validation error (delta[1])
cross_validation_fits_placeholder[i] <- cv_result$delta[1]
cross_validation_fits_placeholder <- numeric(10) # Initialize placeholder for results
# Perform cross-validation
for (i in 1:10) {
# Dynamically create the formula
formula <- as.formula(paste("price ~ poly(n_beds,", i, ")"))
# Fit the model using glm
model <- glm(price ~ poly(n_beds, i), data = airbnbsmall)
# Perform cross-validation
cv_result <- cv.glm(airbnbsmall, model, K = 10)
# Store the cross-validation error (delta[1])
cross_validation_fits_placeholder[i] <- cv_result$delta[1]
}
# Display results
print(cross_validation_fits_placeholder)
attach(airbnbsmall)
#1 linear regression
lm.fit <- glm(price ~ n_beds, data = airbnbsmall, subset = train)
rmse_1 <- mean((price - predict(lm.fit, airbnbsmall))[-train]^2) %>%
round(digits = 2)
print (paste("The RMSE for the linear regression is", rmse_1))
#2
lm.fit2 <- glm(price ~ poly(n_beds, 2), data = airbnbsmall, subset = train)
rmse_2 <- mean((price - predict(lm.fit2, airbnbsmall))[-train]^2)
rmse_2 <- round(rmse_2, digits = 2)
print (paste("The RMSE for the quadratic regression is", rmse_2))
#3
lm.fit3 <- glm(price ~ poly(n_beds, 3), data = airbnbsmall, subset = train)
rmse_3 <- mean((price - predict(lm.fit3, airbnbsmall))[-train]^2) %>%
round(digits = 2)
print (paste("The RMSE for the cubic regression is", rmse_3))
attach(airbnbsmall)
#1 linear regression
model_1 <- glm(price ~ n_beds, data = airbnbsmall, subset = train)
rmse_1 <- mean((price - predict(model_1, airbnbsmall))[-train]^2)
rmse_1 <- round(rmse_1, digits = 2)
print (paste("The RMSE for the linear regression is", rmse_1))
#2
model_2 <- glm(price ~ poly(n_beds, 2), data = airbnbsmall, subset = train)
rmse_2 <- mean((price - predict(model_2, airbnbsmall))[-train]^2)
rmse_2 <- round(rmse_2, digits = 2)
print (paste("The RMSE for the quadratic regression is", rmse_2))
#3
model_3 <- glm(price ~ poly(n_beds, 3), data = airbnbsmall, subset = train)
rmse_3 <- mean((price - predict(model_3, airbnbsmall))[-train]^2)
rmse_3 <- round(rmse_3, digits = 2)
print (paste("The RMSE for the cubic regression is", rmse_3))
library(boot) #library needed for cross-validation
set.seed(111) # to make the example reproducible, we set a fixed number
cross_validation_fits_placeholder <- numeric(5) # Initialize placeholder for results
for (i in 1:5) {
model <- glm(price ~ poly(n_review_scores_rating, i), data = airbnbsmall)
cross_validation_fits_placeholder[i] <- cv.glm(airbnbsmall, model, K = 5)$delta[1]
}
cross_validation_fits_placeholder
library(glmnet)
install.packages("glmnet")
install.packages("glmnet")
library(glmnet)
# Drop any rows the contain missing values
airbnbsmall <- na.omit(airbnbsmall)
# gmlnet() needs the variables as vector/matrix input
# X is a matrix that contains all variables from the regression of all variables o
# y contains the endogenous variable
X = model.matrix(price ~ ., airbnbsmall)[, -1]
y = airbnbsmall$price
fit_ridge_cv = cv.glmnet(X, y, alpha = 0)
plot(fit_ridge_cv)
install.packages("glmnet")
library(glmnet)
# Drop any rows the contain missing values
airbnbsmall <- na.omit(airbnbsmall)
# gmlnet() needs the variables as vector/matrix input
# X is a matrix that contains all variables from the regression of all variables o
# y contains the endogenous variable
X = model.matrix(price ~ ., airbnbsmall)[, -1]
y = airbnbsmall$price
fit_ridge_cv = cv.glmnet(X, y, alpha = 0)
plot(fit_ridge_cv)
bestlambda <- fit_ridge_cv$lambda.min
print(bestlambda)
coef(fit_ridge_cv, s = "lambda.min")
install.packages("glmnet")
#install.packages("glmnet")
library(glmnet)
# Drop any rows the contain missing values
airbnbsmall <- na.omit(airbnbsmall)
# gmlnet() needs the variables as vector/matrix input
# X is a matrix that contains all variables from the regression of all variables o
# y contains the endogenous variable
X = model.matrix(price ~ ., airbnbsmall)[, -1]
y = airbnbsmall$price
fit_ridge_cv = cv.glmnet(X, y, alpha = 0)
plot(fit_ridge_cv)
bestlambda <- fit_ridge_cv$lambda.min
print(bestlambda)
coef(fit_ridge_cv, s = "lambda.min")
#install.packages("glmnet")
library(glmnet)
# Drop any rows the contain missing values
airbnbsmall <- na.omit(airbnbsmall)
# gmlnet() needs the variables as vector/matrix input
# X is a matrix that contains all variables from the regression of all variables o
# y contains the endogenous variable
X = model.matrix(price ~ ., airbnbsmall)[, -1]
y = airbnbsmall$price
fit_ridge_cv = cv.glmnet(X, y, alpha = 0)
#plot(fit_ridge_cv)
bestlambda <- fit_ridge_cv$lambda.min
print(bestlambda)
coef(fit_ridge_cv, s = "lambda.min")
# Retrieve the coefficients at lambda.min
coefficients <- coef(fit_ridge_cv, s = "lambda.min")
# Convert coefficients to a numeric vector and round to 2 decimal points
rounded_coefficients <- round(as.numeric(coefficients), 2)
# Print the rounded coefficients
print(rounded_coefficients)
# Retrieve the coefficients at lambda.min
coefficients <- coef(fit_ridge_cv, s = "lambda.min")
print(coefficients)
# Convert coefficients to a numeric vector and round to 2 decimal points
rounded_coefficients <- round(as.numeric(coefficients), 2)
# Print the rounded coefficients
print(rounded_coefficients)
View(coefficients)
# Convert coefficients to a named numeric vector
coefficients_named <- round(as.numeric(coefficients), 2)
names(coefficients_named) <- rownames(coefficients)
# Print the named coefficients
print(coefficients_named)
#install.packages("glmnet")
library(glmnet)
# Drop any rows the contain missing values
airbnbsmall <- na.omit(airbnbsmall)
# gmlnet() needs the variables as vector/matrix input
# X is a matrix that contains all variables from the regression of all variables o
# y contains the endogenous variable
n_cols <- grep("^n-", names(airbnbsmall), value = TRUE)
X = model.matrix(price ~ ., airbnbsmall[, c("price", n_cols)])[, -1]
#install.packages("glmnet")
library(glmnet)
# Drop any rows the contain missing values
airbnbsmall <- na.omit(airbnbsmall)
# gmlnet() needs the variables as vector/matrix input
# X is a matrix that contains all variables from the regression of all variables o
# y contains the endogenous variable
n_cols <- grep("^n-", names(airbnbsmall), value = TRUE)
X = model.matrix(price ~ ., airbnbsmall[, c("price", n_cols)])[, -1]
#install.packages("glmnet")
library(glmnet)
# Drop any rows the contain missing values
airbnbsmall <- na.omit(airbnbsmall)
# gmlnet() needs the variables as vector/matrix input
# X is a matrix that contains all variables from the regression of all variables o
# y contains the endogenous variable
n_cols <- grep("^n_", names(airbnbsmall), value = TRUE)
print(n_cols)
X = model.matrix(price ~ ., airbnbsmall[, c("price", n_cols)])[, -1]
y = airbnbsmall$price
fit_ridge_cv = cv.glmnet(X, y, alpha = 0)
#plot(fit_ridge_cv)
bestlambda <- fit_ridge_cv$lambda.min
print(bestlambda)
# Convert coefficients to a named numeric vector
coefficients_named <- round(as.numeric(coefficients), 2)
names(coefficients_named) <- rownames(coefficients)
# Print the named coefficients
print(coefficients_named)
#install.packages("glmnet")
library(glmnet)
# Drop any rows the contain missing values
airbnbsmall <- na.omit(airbnbsmall)
# select columns with continuous variables starting with "n_"
n_cols <- grep("^n_", names(airbnbsmall), value = TRUE)
print(paste("Continuous variables in airbnbsmall:", n_cols))
# gmlnet() needs the variables as vector/matrix input
# X is a matrix that contains all variables from the regression of all continuous variables
X = model.matrix(price ~ ., airbnbsmall[, c("price", n_cols)])[, -1]
# y contains the endogenous variable
y = airbnbsmall$price
fit_ridge_cv = cv.glmnet(X, y, alpha = 0)
#plot(fit_ridge_cv)
bestlambda <- fit_ridge_cv$lambda.min
print(bestlambda)
coefficients <- coef(fit_ridge_cv, s = "lambda.min")
# Convert coefficients to a named numeric vector
coefficients_named <- round(as.numeric(coefficients), 2)
names(coefficients_named) <- rownames(coefficients)
# Print the named coefficients
print(coefficients_named)
coefficients <- coef(fit_ridge_cv, s = "lambda.min")
# Convert coefficients to a named numeric vector
coefficients_named <- round(as.numeric(coefficients), 2)
names(coefficients_named) <- rownames(coefficients)
# Print the named coefficients
print(coefficients_named)
cat("Cross-validation error at lambda.min:", fit_ridge$cvm[fit_ridge$lambda == fit_ridge$lambda.min], "\n")
coefficients <- coef(fit_ridge_cv, s = "lambda.min")
# Convert coefficients to a named numeric vector
coefficients_named <- round(as.numeric(coefficients), 2)
names(coefficients_named) <- rownames(coefficients)
# Print the named coefficients
print(coefficients_named)
cat("Cross-validation error at lambda.min:", fit_ridge_cv$cvm[fit_ridge_cv$lambda == fit_ridge_cv$lambda.min], "\n")
#install.packages("glmnet")
library(glmnet)
# Drop any rows the contain missing values
airbnbsmall <- na.omit(airbnbsmall)
# select columns with continuous variables starting with "n_"
n_cols <- grep("^n_", names(airbnbsmall), value = TRUE)
print(paste("Continuous variables in airbnbsmall:", n_cols))
# gmlnet() needs the variables as vector/matrix input
# X is a matrix that contains all variables from the regression of all continuous variables
X = model.matrix(price ~ ., airbnbsmall[, c("price", n_cols)])[, -1]
# y contains the endogenous variable
y = airbnbsmall$price
fit_ridge_cv = cv.glmnet(X, y, alpha = 0)
plot(fit_ridge_cv)
fit_lasso = cv.glmnet(X, y, alpha = 1)
plot(fit_lasso)
bestlambda <- fit_lasso_cv$lambda.min
fit_lasso_cv = cv.glmnet(X, y, alpha = 1)
plot(fit_lasso_cv)
bestlambda <- fit_lasso_cv$lambda.min
print(bestlambda)
coefficients <- coef(fit_lasso_cv, s = "lambda.min")
# Convert coefficients to a named numeric vector
coefficients_named <- round(as.numeric(coefficients), 2)
names(coefficients_named) <- rownames(coefficients)
# Print the named coefficients
print(coefficients_named)
cat("Cross-validation error at lambda.min:", fit_lasso_cv$cvm[fit_lasso_cv$lambda == fit_lasso_cv$lambda.min], "\n")
#install.packages("glmnet")
library(glmnet)
# Drop any rows the contain missing values
airbnbsmall <- na.omit(airbnbsmall)
# select columns with continuous variables starting with "n_"
n_cols <- grep("^n_", names(airbnbsmall), value = TRUE)
print(paste("Continuous variables in airbnbsmall:", n_cols))
# gmlnet() needs the variables as vector/matrix input
# X is a matrix that contains all variables from the regression of all continuous variables
X = model.matrix(price ~ ., airbnbsmall[, c("price", n_cols)])[, -1]
# y contains the endogenous variable
y = airbnbsmall$price
fit_ridge_cv = cv.glmnet(X, y, nfolds=10, alpha = 0)
plot(fit_ridge_cv)
#install.packages("glmnet")
library(glmnet)
# Drop any rows the contain missing values
airbnbsmall <- na.omit(airbnbsmall)
# select columns with continuous variables starting with "n_"
n_cols <- grep("^n_", names(airbnbsmall), value = TRUE)
print(paste("Continuous variables in airbnbsmall:", n_cols))
# gmlnet() needs the variables as vector/matrix input
# X is a matrix that contains all variables from the regression of all continuous variables
X = model.matrix(price ~ ., airbnbsmall[, c("price", n_cols)])[, -1]
# y contains the endogenous variable
y = airbnbsmall$price
fit_ridge_cv = cv.glmnet(X, y, nfolds=10, alpha = 0)
plot(fit_ridge_cv)
bestlambda <- fit_lasso_cv$lambda.min
print(bestlambda)
coefficients <- coef(fit_lasso_cv, s = "lambda.min")
# Convert coefficients to a named numeric vector
coefficients_named <- round(as.numeric(coefficients), 2)
names(coefficients_named) <- rownames(coefficients)
# Print the named coefficients
print(coefficients_named)
cat("Cross-validation error at lambda.min:", fit_lasso_cv$cvm[fit_lasso_cv$lambda == fit_lasso_cv$lambda.min], "\n")
bestlambda <- fit_lasso_cv$lambda.min
cat("Best lambda value for Lasso regression is:", bestlambda)
coefficients <- coef(fit_lasso_cv, s = "lambda.min")
# Convert coefficients to a named numeric vector
coefficients_named <- round(as.numeric(coefficients), 2)
names(coefficients_named) <- rownames(coefficients)
# Print the named coefficients
print(coefficients_named)
cat("Cross-validation error at lambda.min:", fit_lasso_cv$cvm[fit_lasso_cv$lambda == fit_lasso_cv$lambda.min], "\n")
bestlambda <- fit_lasso_cv$lambda.min
cat("Best lambda value for Lasso regression is:", bestlambda, "\n")
coefficients <- coef(fit_lasso_cv, s = "lambda.min")
# Convert coefficients to a named numeric vector
coefficients_named <- round(as.numeric(coefficients), 2)
names(coefficients_named) <- rownames(coefficients)
# Print the named coefficients
cat("X variables and their coefficients:", coefficients_named, "\n")
cat("Cross-validation error at lambda.min:", fit_lasso_cv$cvm[fit_lasso_cv$lambda == fit_lasso_cv$lambda.min], "\n")
bestlambda <- fit_lasso_cv$lambda.min
cat("Best lambda value for Lasso regression is:", bestlambda, "\n")
coefficients <- coef(fit_lasso_cv, s = "lambda.min")
# Convert coefficients to a named numeric vector
coefficients_named <- round(as.numeric(coefficients), 2)
names(coefficients_named) <- rownames(coefficients)
# Print the named coefficients
cat("X variables and their coefficients: \n")
print(coefficients_named)
cat("Cross-validation error at lambda.min:", fit_lasso_cv$cvm[fit_lasso_cv$lambda == fit_lasso_cv$lambda.min], "\n")
bestlambda <- fit_ridge_cv$lambda.min
cat("Best lambda value for Ridge regression is:", bestlambda, "\n")
coefficients <- coef(fit_ridge_cv, s = "lambda.min")
# Convert coefficients to a named numeric vector
coefficients_named <- round(as.numeric(coefficients), 2)
names(coefficients_named) <- rownames(coefficients)
# Print the named coefficients
cat("X variables and their coefficients: \n")
print(coefficients_named)
cat("Cross-validation error at lambda.min:", fit_ridge_cv$cvm[fit_ridge_cv$lambda == fit_ridge_cv$lambda.min], "\n")
library(sozoekds)
set.seed(111) # to make the example reproducible, we set a fixed number
airbnbsmall <- airbnbsmall
#find out number of obervations, 80% in training dataset, 20% in test dataset
n_row <- as.numeric(nrow(airbnbsmall))
print(n_row)
#install.packages("glmnet")
library(glmnet)
# Drop any rows the contain missing values
airbnbsmall <- na.omit(airbnbsmall)
#install.packages("glmnet")
library(glmnet)
data(mtcars) #pre-installed dataset
head(mtcars)
# Target variable
y <- mtcars$mpg
# All other columns as predictors
x <- as.matrix(mtcars[, -which(names(mtcars) == "mpg")])
# Check the matrix
head(x)
# Ridge Regression with glmnet
ridge_model <- glmnet(x, y, alpha = 0)
# Display summary of the model
print(ridge_model)
set.seed(123) # For reproducibility
cv_model <- cv.glmnet(x, y, alpha = 0)
# Plot of cross-validation
plot(cv_model)
# Find the optimal lambda value
best_lambda <- cv_model$lambda.min
print(best_lambda)
# Retrieve coefficients using the optimal lambda value
ridge_coefficients <- coef(cv_model, s = "lambda.min")
# Display the coefficients
print(ridge_coefficients)
#install.packages("glmnet")
library(glmnet)
# Drop any rows the contain missing values
airbnbsmall <- na.omit(airbnbsmall)
#install.packages("glmnet")
library(sozoekds)
library(glmnet)
# Drop any rows the contain missing values
airbnbsmall <- na.omit(airbnbsmall)
# select columns with continuous variables starting with "n_"
n_cols <- grep("^n_", names(airbnbsmall), value = TRUE)
print(paste("Continuous variables in airbnbsmall:", n_cols))
# gmlnet() needs the variables as vector/matrix input
# X is a matrix that contains all variables from the regression of all continuous variables
X = model.matrix(price ~ ., airbnbsmall[, c("price", n_cols)])[, -1]
# y contains the endogenous variable
y = airbnbsmall$price
#install.packages("glmnet")
library(sozoekds)
library(glmnet)
# Drop any rows the contain missing values
airbnbsmall <- na.omit(airbnbsmall)
# select columns with continuous variables starting with "n_"
n_cols <- grep("^n_", names(airbnbsmall), value = TRUE)
print(paste("Continuous variables in airbnbsmall:", n_cols))
# gmlnet() needs the variables as vector/matrix input
# X is a matrix that contains all variables from the regression of all continuous variables
X = model.matrix(price ~ ., airbnbsmall[, c("price", n_cols)])[, -1]
# y contains the endogenous variable
y = airbnbsmall$price
fit_ridge_cv = cv.glmnet(X, y, nfolds=10, alpha = 0)
plot(fit_ridge_cv)
bestlambda <- fit_ridge_cv$lambda.min
cat("Best lambda value for Ridge regression is:", bestlambda, "\n")
coefficients <- coef(fit_ridge_cv, s = "lambda.min")
# Convert coefficients to a named numeric vector
coefficients_named <- round(as.numeric(coefficients), 2)
names(coefficients_named) <- rownames(coefficients)
# Print the named coefficients
cat("X variables and their coefficients: \n")
print(coefficients_named)
cat("Cross-validation error at lambda.min:", fit_ridge_cv$cvm[fit_ridge_cv$lambda == fit_ridge_cv$lambda.min], "\n")
fit_lasso_cv = cv.glmnet(X, y, nfolds=10, alpha = 1)
plot(fit_lasso_cv)
bestlambda <- fit_lasso_cv$lambda.min
cat("Best lambda value for Lasso regression is:", bestlambda, "\n")
coefficients <- coef(fit_lasso_cv, s = "lambda.min")
# Convert coefficients to a named numeric vector
coefficients_named <- round(as.numeric(coefficients), 2)
names(coefficients_named) <- rownames(coefficients)
# Print the named coefficients
cat("X variables and their coefficients: \n")
print(coefficients_named)
cat("Cross-validation error at lambda.min:", fit_lasso_cv$cvm[fit_lasso_cv$lambda == fit_lasso_cv$lambda.min], "\n")
